{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to optimize the Meta Llama-3 70B Amazon JumpStart model for inference using Amazon SageMaker model optimization jobs\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to apply state-of-the-art optimization techniques to an Amazon JumpStart model (JumpStart model ID: `meta-textgeneration-llama-3-70b`) using Amazon SageMaker ahead-of-time (AOT) model optimization capabilities. Each example includes the deployment of the optimized model to an Amazon SageMaker endpoint. In all cases, the inference image will be the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "You will successively:\n",
    "1. Deploy a pre-optimized variant of the Amazon JumpStart model with speculative decoding enabled (using SageMaker provided draft model). For popular models, the JumpStart team indeed selects and applies the best optimization configurations for you.\n",
    "\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.p4d.24xlarge` and `ml.inf2.48xlarge` instance types required for the tutorials are available in your AWS Region.\n",
    "* Make sure that the value of your \"ml.p4d.24xlarge for endpoint usage\" and \"ml.inf2.48xlarge for endpoint usage\" Amazon SageMaker service quotas allow you to deploy at least one Amazon SageMaker endpoint using these instance types.\n",
    "\n",
    "This notebook leverages the [Model Builder Class](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-modelbuilder-creation.html) within the [`sagemaker` Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to abstract out container and model server management/tuning. Via the Model Builder Class you can easily interact with JumpStart Models, HuggingFace Hub Models, and also custom models via pointing towards an S3 path with your Model Data. For this sample we will focus on the JumpStart Optimization path.\n",
    "\n",
    "### License agreement\n",
    "* This model is under the Meta license, please refer to the original model card.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#)\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.225.0 \n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker>=2.225.0 boto3 huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.session import Session\n",
    "import logging\n",
    "import huggingface_hub\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b6d0b-5c6c-4014-b836-df0268e11feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "\n",
    "artifacts_bucket_name = sagemaker_session.default_bucket()\n",
    "execution_role_arn = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "js_model_id = \"meta-textgeneration-llama-3-70b\"\n",
    "gpu_instance_type = \"ml.p4d.24xlarge\"\n",
    "neuron_instance_type = \"ml.inf2.48xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb26871-315d-425f-a5d2-b05627191700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = \"Hello, I'm a language model, and I'm here to help you with your English.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": \"Hello, I'm a language model,\",\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "\n",
    "sample_output = [{\"generated_text\": response}]\n",
    "\n",
    "schema_builder = SchemaBuilder(sample_input, sample_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81838b96-ca02-4892-b861-8cb0634fb167",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Deploy a pre-optimized deployment configuration with speculative decoding (SageMaker provided draft model)\n",
    "The `meta-textgeneration-llama-3-70b` JumpStart model is available with multiple pre-optimized deployment configuration. Optimized model artifacts for each configuration have already been created by the JumpStart team and a readily available for deployment. In this section, you will deploy one of theses pre-optimized configuration to an Amazon SageMaker endpoint. \n",
    "\n",
    "### What is speculative decoding?\n",
    "Speculative decoding is an inference optimization technique introduced by [Y. Leviathan et al. (ICML 2023)](https://arxiv.org/abs/2211.17192) used to accelerate the decoding process of large and therefore slow LLMs for latency-critical applications. The key idea is to use a smaller, less powerful but faster model called the ***draft model*** to generate candidate tokens that get validated by the larger, more powerful but slower model called the ***target model***. At each iteration, the draft model generates $K>1$ candidate tokens. Then, using a single forward pass of the larger target model, none, part, or all candidate tokens get accepted. The more aligned the selected draft model is with the target model, the better guesses it makes, the higher candidate token acceptance rate and therefore the higher the speed ups. The larger the size gap between the target and the draft model, the largest the potential speedups.\n",
    "\n",
    "Let's start by creating a `ModelBuilder` instance for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490309f-da98-4489-8bab-d1ffde767370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    "    log_level=logging.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294805b8-f056-4a24-9d10-df32039d1193",
   "metadata": {},
   "source": [
    "For each optimization configuration, the JumpStart team has computed key performance metrics such as time-to-first-token (TTFT) latency and throughput for multiple hardwares and concurrent invocation intensities. Let's visualize these metrics using the `display_benchmark_metrics` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eedd83-3104-4820-bb5c-6164b9d0477f",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder.display_benchmark_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f84c42-d578-4136-9e86-34058e134707",
   "metadata": {},
   "source": [
    "Now, let's pick and deploy the `lmi-optimized` pre-optimized configuration to a `ml.p4d.24xlarge` instance. The `lmi-optimized` configuration enables speculative decoding. In this configuration, a SageMaker provided draft model is used. Therefore, you don't have to supply a draft model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bb346-4108-4721-86bc-c754d59c4bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder.set_deployment_config(config_name=\"lmi-optimized\", instance_type=\"ml.g5.48xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35232c11-eb78-4521-ae60-243bdc7e1666",
   "metadata": {},
   "source": [
    "Currently set deployment configuration can be visualized using the `get_deployment_config` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e44a7-cce1-4417-b701-6895638fa42e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder.get_deployment_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad37a7b-e2cc-45af-bf27-ea6c1aa258ba",
   "metadata": {},
   "source": [
    "Now, let's build the `Model` instance and use it to deploy the selected optimized configuration. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972aea37-4a16-4eee-b3ad-33b1077cbcbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a2527-a189-4b26-b579-bae031f69248",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = optimized_model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7fb08-3ff0-40c5-8a4e-9a026d7fba62",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method as see below. Once you have tested it, you can use this endpoint to connect to the generative ai application builder on AWS to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8249e-9e8a-48d1-9dd2-2a31d664d954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# How to optimize the Meta Llama-3 70B Amazon JumpStart model for inference using Amazon SageMaker model optimization jobs\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to apply state-of-the-art optimization techniques to an Amazon JumpStart model (JumpStart model ID: `meta-textgeneration-llama-3-70b`) using Amazon SageMaker ahead-of-time (AOT) model optimization capabilities. Each example includes the deployment of the optimized model to an Amazon SageMaker endpoint. In all cases, the inference image will be the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images features a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/). \n",
    "\n",
    "You will successively:\n",
    "1. Deploy a pre-optimized variant of the Amazon JumpStart model with speculative decoding enabled (using SageMaker provided draft model). For popular models, the JumpStart team indeed selects and applies the best optimization configurations for you.\n",
    "\n",
    "\n",
    "**Notices:**\n",
    "* Make sure that the `ml.p4d.24xlarge` and `ml.inf2.48xlarge` instance types required for this tutorial are available in your AWS Region.\n",
    "* Make sure that the value of your \"ml.p4d.24xlarge for endpoint usage\" and \"ml.inf2.48xlarge for endpoint usage\" Amazon SageMaker service quotas allow you to deploy at least one Amazon SageMaker endpoint using these instance types.\n",
    "\n",
    "This notebook leverages the [Model Builder Class](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-modelbuilder-creation.html) within the [`sagemaker` Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html) to abstract out container and model server management/tuning. Via the Model Builder Class you can easily interact with JumpStart Models, HuggingFace Hub Models, and also custom models via pointing towards an S3 path with your Model Data. For this sample we will focus on the JumpStart Optimization path.\n",
    "\n",
    "### License agreement\n",
    "* This model is under the Meta license, please refer to the original model card.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html#)\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.225.0 \n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker>=2.225.0 boto3 huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.session import Session\n",
    "import logging\n",
    "import huggingface_hub\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b6d0b-5c6c-4014-b836-df0268e11feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "\n",
    "artifacts_bucket_name = sagemaker_session.default_bucket()\n",
    "execution_role_arn = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "js_model_id = \"meta-textgeneration-llama-3-70b\"\n",
    "gpu_instance_type = \"ml.p4d.24xlarge\"\n",
    "neuron_instance_type = \"ml.inf2.48xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb26871-315d-425f-a5d2-b05627191700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = \"Hello, I'm a language model, and I'm here to help you with your English.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": \"Hello, I'm a language model,\",\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "\n",
    "sample_output = [{\"generated_text\": response}]\n",
    "\n",
    "schema_builder = SchemaBuilder(sample_input, sample_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4550542-4e10-452f-bca7-a1f1a24a28f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run optimization job to compile the model using the AWS Neuron Compiler then deploy the compiled model to an Inferentia 2 endpoint\n",
    "In this section, you will use an Amazon SageMaker optimization job as a managed model compiler to compile the `meta-textgeneration-llama-3-70b` JumpStart model for [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/) hardware. The optimization job allows you to decouple compilation from deployment. The model is compiled once while the compiled artifacts can be deployed many times. In other words, the compilation overhead is paid once instead of occuring upon each deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75309273-2780-44c7-acfe-4879029e00cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d28124-6dc0-4643-9472-86cc7ab75d05",
   "metadata": {},
   "source": [
    "Now let's compile the model for Inferentia 2. This operation takes around 40min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26e543-6951-40dd-801a-93984e3ffaf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimized_model = model_builder.optimize(\n",
    "    instance_type=neuron_instance_type,\n",
    "    accept_eula=True,\n",
    "    compilation_config={\n",
    "        \"OverrideEnvironment\": {\n",
    "            \"OPTION_TENSOR_PARALLEL_DEGREE\": \"24\",\n",
    "            \"OPTION_N_POSITIONS\": \"8192\",\n",
    "            \"OPTION_DTYPE\": \"fp16\",\n",
    "            \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "            \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
    "            \"OPTION_NEURON_OPTIMIZE_LEVEL\": \"2\",\n",
    "        }\n",
    "    },\n",
    "    output_path=f\"s3://{artifacts_bucket_name}/neuron/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811e8b3-f145-4d08-85ee-3a8d797d5ae6",
   "metadata": {},
   "source": [
    "Now let's deploy the compiled model to an Amazon SageMaker endpoint powered by Inferentia 2 hardware. This operation may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c8835-081d-457f-aabd-53dc948df0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = optimized_model.deploy(accept_eula=True, model_data_download_timeout=3600, volume_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221bcb2-b16c-4c2b-8e63-9f684ca1e9e0",
   "metadata": {},
   "source": [
    "Once the deployment has finished successfully, you can send queries to the model by simply using the predictor's `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c939d2-2b37-4377-92bf-d54e2b3bb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23da2a4-b113-44d4-9a59-cbfdeff7a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
